#!/usr/bin/env python3
"""çˆ¬è™«åŠŸèƒ½æ¼”ç¤ºè„šæœ¬"""
import sys
import os
import asyncio
import logging
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥çˆ¬è™«
from app.crawlers.nga_crawler import NGACrawler
from app.crawlers.zhihu_crawler import ZhihuCrawler
from app.crawlers.weibo_crawler import WeiboCrawler
from app.crawlers.bilibili_crawler import BiliBiliCrawler
from app.crawlers.toutiao_crawler import ToutiaoCrawler
from app.crawlers.hupu_crawler import HupuCrawler
from app.crawlers.ithome_crawler import ITHomeCrawler
from app.crawlers.zol_crawler import ZOLCrawler

async def test_crawler(crawler_class, name):
    """æµ‹è¯•å•ä¸ªçˆ¬è™«"""
    print(f"\n{'='*20} æµ‹è¯• {name} {'='*20}")
    try:
        crawler = crawler_class()
        print(f"âœ“ {name} å®ä¾‹åŒ–æˆåŠŸ")
        print(f"  å¹³å°: {crawler.platform}")
        print(f"  åˆ†ç±»: {crawler.category}")
        
        # å°è¯•çˆ¬å–æ•°æ®ï¼ˆé™åˆ¶æ•°é‡é¿å…è¿‡å¤šè¾“å‡ºï¼‰
        print(f"\nå¼€å§‹çˆ¬å– {name} æ•°æ®...")
        async with crawler:
            items = await crawler.crawl()
        
        if items:
            print(f"âœ“ æˆåŠŸçˆ¬å–åˆ° {len(items)} æ¡æ•°æ®")
            # æ˜¾ç¤ºå‰3æ¡æ•°æ®ä½œä¸ºç¤ºä¾‹
            for i, item in enumerate(items[:3]):
                print(f"\nç¬¬ {i+1} æ¡:")
                print(f"  æ ‡é¢˜: {item.title[:50]}{'...' if len(item.title) > 50 else ''}")
                print(f"  æ’å: {item.rank}")
                print(f"  é“¾æ¥: {item.url[:80]}{'...' if len(item.url) > 80 else ''}")
                if item.hot_value:
                    print(f"  çƒ­åº¦: {item.hot_value}")
                if item.author:
                    print(f"  ä½œè€…: {item.author}")
        else:
            print("âš ï¸ æœªè·å–åˆ°æ•°æ®")
            
    except Exception as e:
        print(f"âœ— {name} æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹çˆ¬è™«åŠŸèƒ½æ¼”ç¤º")
    print("æ³¨æ„: ç”±äºç½‘ç»œé™åˆ¶ï¼Œéƒ¨åˆ†çˆ¬è™«å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ")
    
    # è¦æµ‹è¯•çš„çˆ¬è™«åˆ—è¡¨
    crawlers = [
        (NGACrawler, "NGAæ‚è°ˆ"),
        (ZhihuCrawler, "çŸ¥ä¹çƒ­æ¦œ"),
        (WeiboCrawler, "å¾®åšçƒ­æœ"),
        (BiliBiliCrawler, "Bç«™çƒ­æ¦œ"),
        (ToutiaoCrawler, "ä»Šæ—¥å¤´æ¡"),
        (HupuCrawler, "è™æ‰‘æ­¥è¡Œè¡—"),
        (ITHomeCrawler, "ITä¹‹å®¶"),
        (ZOLCrawler, "ä¸­å…³æ‘åœ¨çº¿")
    ]
    
    success_count = 0
    total_items = 0
    
    for crawler_class, name in crawlers:
        try:
            await test_crawler(crawler_class, name)
            success_count += 1
        except Exception as e:
            print(f"\nâœ— {name} æ•´ä½“æµ‹è¯•å¤±è´¥: {e}")
        
        # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
        await asyncio.sleep(1)
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š æµ‹è¯•æ€»ç»“:")
    print(f"  æˆåŠŸæµ‹è¯•: {success_count}/{len(crawlers)} ä¸ªçˆ¬è™«")
    print(f"\nğŸ‰ çˆ¬è™«ç³»ç»Ÿå·²æˆåŠŸæ‰©å±•åˆ° {len(crawlers)} ä¸ªå¹³å°ï¼")
    print(f"\nğŸ“ æ”¯æŒçš„å¹³å°:")
    for _, name in crawlers:
        print(f"  - {name}")
    
    print(f"\nğŸ’¡ ä½¿ç”¨è¯´æ˜:")
    print(f"  1. æ¯ä¸ªçˆ¬è™«éƒ½æ”¯æŒè·å–æœ€å¤š30æ¡çƒ­æ¦œæ•°æ®")
    print(f"  2. å‰ç«¯ç•Œé¢æ”¯æŒæ»‘åŠ¨æµè§ˆå’Œå¹³å°ç­›é€‰")
    print(f"  3. ç³»ç»Ÿä¼šè‡ªåŠ¨å¤„ç†ç½‘ç»œå¼‚å¸¸å’Œæ•°æ®è§£æé”™è¯¯")
    print(f"  4. å¯ä»¥é€šè¿‡APIæ¥å£ /api/v1/hot è·å–æ‰€æœ‰å¹³å°æ•°æ®")

if __name__ == "__main__":
    asyncio.run(main())
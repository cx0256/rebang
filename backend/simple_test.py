#!/usr/bin/env python3
"""ç®€åŒ–çš„çˆ¬è™«æµ‹è¯•è„šæœ¬"""
import sys
import os
import logging
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

print("å¼€å§‹æµ‹è¯•çˆ¬è™«æ¨¡å—å¯¼å…¥...")
print("="*50)

# æµ‹è¯•åŸºç¡€æ¨¡å—
try:
    from app.crawlers.base import BaseCrawler, HotItem
    print("âœ“ BaseCrawler å’Œ HotItem å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âœ— BaseCrawler å¯¼å…¥å¤±è´¥: {e}")
    print("ç¼ºå°‘ä¾èµ–åº“ï¼Œè¯·å®‰è£…: pip install aiohttp beautifulsoup4")
    sys.exit(1)

# æµ‹è¯•å„ä¸ªçˆ¬è™«
crawlers_to_test = [
    ('NGACrawler', 'app.crawlers.nga_crawler'),
    ('ZhihuCrawler', 'app.crawlers.zhihu_crawler'),
    ('WeiboCrawler', 'app.crawlers.weibo_crawler'),
    ('BiliBiliCrawler', 'app.crawlers.bilibili_crawler'),
    ('ToutiaoCrawler', 'app.crawlers.toutiao_crawler'),
    ('HupuCrawler', 'app.crawlers.hupu_crawler'),
    ('ITHomeCrawler', 'app.crawlers.ithome_crawler'),
    ('ZOLCrawler', 'app.crawlers.zol_crawler')
]

success_count = 0
failed_crawlers = []

for crawler_name, module_path in crawlers_to_test:
    try:
        module = __import__(module_path, fromlist=[crawler_name])
        crawler_class = getattr(module, crawler_name)
        print(f"âœ“ {crawler_name} å¯¼å…¥æˆåŠŸ")
        success_count += 1
    except ImportError as e:
        print(f"âœ— {crawler_name} å¯¼å…¥å¤±è´¥: {e}")
        failed_crawlers.append(crawler_name)
    except Exception as e:
        print(f"âœ— {crawler_name} å…¶ä»–é”™è¯¯: {e}")
        failed_crawlers.append(crawler_name)

print("\n" + "="*50)
print(f"æµ‹è¯•ç»“æœ: {success_count}/{len(crawlers_to_test)} ä¸ªçˆ¬è™«å¯¼å…¥æˆåŠŸ")

if failed_crawlers:
    print(f"å¤±è´¥çš„çˆ¬è™«: {', '.join(failed_crawlers)}")
    print("\nå»ºè®®å®‰è£…ç¼ºå¤±çš„ä¾èµ–:")
    print("pip install aiohttp beautifulsoup4 requests")
else:
    print("\nğŸ‰ æ‰€æœ‰çˆ¬è™«æ¨¡å—å¯¼å…¥æˆåŠŸï¼")
    
    # æµ‹è¯•CrawlerManagerï¼ˆå¦‚æœæ²¡æœ‰æ•°æ®åº“ä¾èµ–é—®é¢˜ï¼‰
    try:
        from app.crawlers.crawler_manager import CrawlerManager
        print("\nâœ“ CrawlerManager å¯¼å…¥æˆåŠŸ")
        
        # åˆ›å»ºå®ä¾‹ï¼ˆä¸è¿æ¥æ•°æ®åº“ï¼‰
        manager = CrawlerManager()
        crawlers = manager.get_all_crawlers()
        print(f"âœ“ CrawlerManager åˆå§‹åŒ–æˆåŠŸï¼Œæ³¨å†Œäº† {len(crawlers)} ä¸ªçˆ¬è™«:")
        for name in crawlers.keys():
            print(f"  - {name}")
    except ImportError as e:
        print(f"\nâœ— CrawlerManager å¯¼å…¥å¤±è´¥: {e}")
        print("å¯èƒ½ç¼ºå°‘æ•°æ®åº“ç›¸å…³ä¾èµ–")
    except Exception as e:
        print(f"\nâœ— CrawlerManager åˆå§‹åŒ–å¤±è´¥: {e}")

print("\næµ‹è¯•å®Œæˆï¼")